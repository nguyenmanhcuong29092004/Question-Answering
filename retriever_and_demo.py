# -*- coding: utf-8 -*-
"""Retriever and demo

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/retriever-and-demo-089568cb-e5bb-4b52-aaf3-149cd3143a38.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20240811/auto/storage/goog4_request%26X-Goog-Date%3D20240811T031559Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D25149a49dc030c4bf603a88e597bb964024c235382ffa7cb96333b51eb661646ca1389ebed9755d373a26c41a19208adfc6861a9ca663668141b1e773ec1dcaf80bb0ab5b9cd5f50338d67ddb7b9725007124dfb49303849ba43ea7817a8e3d64467d6a2974d15600c78c5436cbb60dda81f93dd8a315acbd0aff9a2f36e3c1dd419338422c3b6ff2610c0c4b784d6abed0f8cc1ca63c1abb61fdebf610a73dd24bbbbcdbe8d442e53fee8270ca8daa7f4ae215dee6d3d0f331f6129b1d8705c81e0c199f143a13dcacf9e888b782b83c842293a49b52429be5a83befb5006e43ed8a4b6f075b553801a0b565eb55c7d6e2c61b6b1a43233e371ba0aa1e55bda
"""

import streamlit as st
from transformers import pipeline, AutoTokenizer, AutoModel
import torch
from datasets import load_dataset
import numpy as np
import faiss

# Thiết lập thiết bị (GPU nếu có, nếu không sẽ dùng CPU)
device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")

# Tải dataset và lọc dữ liệu
DATASET_NAME = "squad_v2"
raw_datasets = load_dataset(DATASET_NAME, split='train+validation')
raw_datasets = raw_datasets.filter(
    lambda x: len(x['answers']['text']) > 0
)

# Tạo mô hình và tokenizer
MODEL_NAME = "distilbert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)
model = AutoModel.from_pretrained(MODEL_NAME).to(device)

def cls_pooling(model_output):
    return model_output.last_hidden_state[:, 0]

def get_embeddings(text_list):
    # Mã hóa đầu vào văn bản
    encoded_input = tokenizer(
        text_list,
        padding=True,
        truncation=True,
        return_tensors='pt'
    )
    # Di chuyển các tensor đến thiết bị (GPU hoặc CPU)
    encoded_input = {k: v.to(device) for k, v in encoded_input.items()}
    # Lấy đầu ra của mô hình
    model_output = model(**encoded_input)

    # Trả về kết quả từ hàm cls_pooling
    return cls_pooling(model_output)

# Đặt tên cho cột embedding
EMBEDDING_COLUMN = 'question_embedding'

# Sử dụng hàm map để thêm cột embedding vào dataset
embeddings_dataset = raw_datasets.map(
    lambda x: {
        EMBEDDING_COLUMN: get_embeddings(
            [x['question']]
        ).detach().cpu().numpy()[0]
    }
)

# Thêm chỉ mục FAISS vào dataset với cột embedding
embeddings_dataset.add_faiss_index(column=EMBEDDING_COLUMN)

# Khởi tạo mô hình hỏi-đáp đã huấn luyện
QA_MODEL_NAME = 'NMC-29092004/distilbert-finetuned-squadv2-2'
pipe = pipeline('question-answering', model=QA_MODEL_NAME, device=0 if torch.cuda.is_available() else -1)

# Giao diện Streamlit
st.title("Demo Hỏi-Đáp")

input_question = st.text_input("Nhập câu hỏi của bạn:", value="When did Beyonce start becoming popular?")

if input_question:
    input_quest_embedding = get_embeddings([input_question])
    input_quest_embedding = input_quest_embedding.cpu().detach().numpy()

    TOP_K = 5
    scores, samples = embeddings_dataset.get_nearest_examples(
        EMBEDDING_COLUMN, input_quest_embedding, k=TOP_K
    )

    best_answer = None
    best_score = float('inf')

    for idx, score in enumerate(scores):
        question = samples["question"][idx]
        context = samples["context"][idx]
        answer = pipe(
            question=question,
            context=context
        )
        if answer['score'] < best_score:
            best_score = answer['score']
            best_answer = answer

    # Hiển thị kết quả
    st.write("### Kết quả:")
    if best_answer:
        st.write(f"**Best Answer**: {best_answer['answer']}")
        st.write(f"**Score**: {best_answer['score']}")
        st.write(f"**Context**: {context}")